# soacceptqueue is the kernel's backlog queue depth for accepting new TCP
# connections. A larger value should prevent clients from being dropped
# during sudden bursty periods at the expense of more RAM and CPU load.
kern.ipc.soacceptqueue=1024 # (default 128)

# maxsockbuf is the max amount of memory that can be allocated to a socket.
# In practice, this value determines the TCP window scaling factor - the
# number of bytes that can be transmitted without requiring an ACK. If our
# server is not under heavy load, we want a large scaling factor, because
# we can transmit packets as fast as the receiver can process them.
#
# The default maxsockbuf value (2MB) will result in a scaling factor of 6,
# which is ideal for a low-latency gigabit network.
#
# However, my server is on the west coast, on a gigabit connection with
# 100ms of latency to my hometown on the east coast. A scaling factor of 8:
#
#     2^8 x 65,535-byte IP packet size = 16,776,960 bytes
#
# happens to saturates my connection:
#
#   16,776,960 bytes * 8 / .1 sec latency / 10^9 = 1.3421568 Gbps
#
# You remember Bandwidth Delay Product from networking class, right? :-)
#
# A maxsockbuf value of 8MB will yield a scaling factor of 8. To see
# how this is derived, you can check /sys/netinet/tcp_syncache.c.
#
kern.ipc.maxsockbuf=8388608

# sendspace and recvspace are the network buffer sizes *initially* allocated
# to each TCP connection. Bandwidth can be improved by increasing the buffer
# size at the cost of using more kernel memory per connection. Saturating my
# gigabit connection with 100ms latency would require:
#
#  1000 megabits * .1 sec / 8 bits = 12.5 MB
#
# However, just 80 simultaneous connections would immediately consume a
# GIGABYTE of RAM! Most of the traffic to my server is for small, static
# HTML pages and some SSH connections, so I've set a smaller default size:
#
#   256 KB = 256 * 1024 bytes = 262,144 bytes
#
# The kernel will allocate more memory as needed (at a very slight
# performance hit) for the occasional full-throttle transfer of large
# files.
#
net.inet.tcp.sendspace=262144 # (default 32768)
net.inet.tcp.recvspace=262144 # (default 65536)

# sendbuf_max and recvbuf_max control the maximum send and recv buffer sizes
# the kernel will ever allocate for a single TCP connection. I set mine to
# 16 MB, which is slightly higher than the 12.5 MB I calculated above. This
# should let me to saturate my 100ms connection, as well as leave some
# wiggle room to saturate even higher-latency clients.
#
# You should probably make sure these values are at least as large as
# maxsockbuf.
#
net.inet.tcp.sendbuf_max=16777216
net.inet.tcp.recvbuf_max=16777216

# sendbuf_inc and recvbuf_inc control the increments by which the kernel
# increases sendspace and recvspace to sendbuf_max and recvbuf_max,
# respectively. Higher values will cause fewer memory allocations, but may
# result in wasted buffer space.
#
net.inet.tcp.sendbuf_inc=32768   # (default 8192)
net.inet.tcp.recvbuf_inc=65536   # (default 16384)

# increase the localhost buffer space, which may help localhost-only servers
# more efficiently move data to network buffers.
net.local.stream.sendspace=16384 # default (8192)
net.local.stream.recvspace=16384 # default (8192)

# increase the raw IP datagram buffers to the MTU for the localhost
# interface (2^14 bytes = 16384). Thanks, Calomel!
net.inet.raw.maxdgram=16384
net.inet.raw.recvspace=16384

# *** these two settings gave me the most drastic improvement: ***

# TCP Slow Start gradually ramps up the data transmission rate until the
# throughput on the network path has been determined. TCP Appropriate Byte
# Counting (ABC) allows the kernel to increase the window size
# exponentially. According to Calomel, with maxseg set to the default of
# 1460 bytes, setting abc_l_var to 44 allows an increase of about 64 KB per
# step, which happens to be the receive buffer size of most hosts that don't
# support window scaling.
#
net.inet.tcp.abc_l_var=44         # (default 2)

# The TCP initial congestion window determines the *initial* amount of data
# that can be sent over the network before requiring an ACK from the other
# side. The Slow Start algorithm will improve this value over time. A larger
# initcwnd will speed up short, bursty connections. Google recommends 16,
# but according to Calomel, you should also test 44.
net.inet.tcp.initcwnd_segments=44 # (default 10)

# Maximum Segment Size (MSS) specifies the largest amount of data that can
# be placed in a single IPv4 TCP segment. This value is usually equal to:
#
#   MTU (usually 1500) - 20 byte IPv4 header - 20 byte TCP header = 1460
#
# If you have net.inet.tcp.rfc1323 enabled (it is by default on FreeBSD),
# then TCP hosts can negotiate timestamps which increases the TCP headers
# by 12 bytes. So in the case we'll use 1460 - 12  = 1448 bytes.
#
net.inet.tcp.mssdflt=1448 # (default 536)

# The Minimum MSS specifies the smallest amount of data we will send in a
# single IPv4 TCP segment. RFC 6691 requires a minimum value of 576 bytes.
# Subtracting a 20 byte IP header and 20 (or 32, see above) byte TCP header
# gives us:
#
#  576 (minimum MTU) - 20 byte IPv4 header - 32 byte TCP header = 524 bytes.
#
net.inet.tcp.minmss=524 # (default 216)

# use the H-TCP algorithm that we enabled in /boot/loader.conf above.
#net.inet.tcp.cc.algorithm=htcp

# Enable H-TCP's adaptive backoff optimization, which increases buffer
# efficiency along the network path.
#net.inet.tcp.cc.htcp.adaptive_backoff=1

# Enable H-TCP's RTT scaling optimization, which increases fairness between
# flows with different RTTs.
#net.inet.tcp.cc.htcp.rtt_scaling=1

# RFC 6675 improves TCP Fast Recovery when combined with SACK (which is
# enabled by default on FreeBSD - net.inet.tcp.sack.enable)
net.inet.tcp.rfc6675_pipe=1

# Disabling syncookies will give us more TCP features like window scale and
# timestamps at the expense of making us more vulnerable to DoS attacks.
net.inet.tcp.syncookies=0

# disable the TIME_WAIT state for the localhost interface, this should
# result in localhost sockets being freed more quickly.
net.inet.tcp.nolocaltimewait=1

# TSO (and LRO) should be disabled on machines that forward packets. I use
# OpenVPN, sometimes, so I've disabled TSO here. These options can also be
# disabled in /etc/rc.conf using the `-tso -lro` options.
net.inet.tcp.tso=0

# these two values control the number of frames the NIC will accept before
# firing an interrupt. If the queue fills up and the machine is overloaded,
# packets will be dropped. Increasing this value should mitigate packet loss
# in case of a storm of short, bursty packets, but if
# net.inet.ip.intr_queue_drops remains greater than 0, you probably just
# need better hardware.
#
net.inet.ip.intr_queue_maxlen=2048 # (default 256)
net.route.netisr_maxqlen=2048      # (default 256)

# Disable Intel's hardware-based ethernet flow control. Instead we will rely
# on TCP which is peer-based and more fair to each flow.
#dev.igb.0.fc=0 # (default 3)
#dev.igb.1.fc=0 # (default 3)

# Finally, increasing the UFS read ahead value almost always results in better performance. Reboot required
vfs.read_max=128
